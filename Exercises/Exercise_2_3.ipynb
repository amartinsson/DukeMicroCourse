{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import numpy and matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import mini MD modules\n",
    "from miniMD import models\n",
    "from miniMD import samplers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a binary classification problem, for a given set of paired data $(x_i,y_i)_{1 \\leq i \\leq N}$, where $y_i \\in \\mathbb{R}^d$ denotes the predictor variable and $y_i \\in \\{0, 1\\}$ the response variable. Let $X_i$ and $Y_i$ as described above be i.i.d. random variables associated with the observations $(y_i,z_i)$, respectively. A common way to model the dependence between the predictor variable and the response variable in binary classification problems is to assume that the conditional probability of the event $Y_i = 1$ given $X_i$ is described by the values of the logistic function evaluated at a linear transformation of $X_i$, i.e.,\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(Y_i =1|X_i)=f( x_i \\beta^T\\,)\n",
    "$$\n",
    "\n",
    "hence \n",
    "$$\\mathbb{P}(Y_i \\beta^T X_i) = f(Y_i \\beta^T X_i),$$\n",
    "where\n",
    "$$\n",
    "f(s) = \\frac{1}{1+e^{-s}}.\n",
    "$$\n",
    "is the logistic function and the weights w ∈ Rn define the linear transformation. In a Bayesian setup, the weights are considered to be random variables distributed according to a prior distribution, w ∼ π. By Bayes rule, the posterior distribution over w is then found to be,\n",
    "$$\n",
    "p(\\beta | x,y) \\propto \\pi(\\beta) p(y, | x, \\beta) = \\pi(\\beta) \\prod_{i=1}^N f( x_i \\beta^T\\, y_i ). \n",
    "$$\n",
    "Using the posterior distribution we can predict the class label of a new data point as \n",
    "$$\n",
    "\\mathbb{P}[y_{pred} | x_{pred},x,y] = \\int f(x_{pred}^T\\beta) p(\\beta | x,y) d \\beta\n",
    "$$\n",
    "In practice the integral on the left hand side of the equation is intractable. This is were sampling comes into play. One generate a sample $(\\beta^(k))_{1\\leq k \\leq N_{steps}}$ from the posterior distribution and approximate the integral term as \n",
    "$$\n",
    "\\int f(x_{pred}^T\\beta) p(\\beta | x,y) d \\beta  \\approx \\frac{1}{N_{steps}}\\sum_{k=1}^{N_{steps}} f(x_{pred}^T\\beta^{(k)})\n",
    "$$\n",
    "\n",
    "In this exercise we will consider a simple logistic regression classfier using a synthetic dataset of $N$ paired observations $(x_i,y_i)_{1\\leq i \\leq N}$, with $x_i \\in \\mathbb{R}^2$ being the predictor variable and $y_i\\in \\{0,1\\}$ being the class label as a training set. The code below creates such a synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed=11) #Fix seed \n",
    "\n",
    "data_dim = 2 # Dimension of predictor variable\n",
    "Ndata1 = 10  # Number of points with class label 0\n",
    "Ndata2 = 10  # Number of points with class label 1\n",
    "\n",
    "mu1 = np.array([-4,0]) # mean of predictor variables with class label 0\n",
    "mu2 =  np.array([4,0]) # mean of predictor variables with class label 1\n",
    "cov1 = np.eye(data_dim) # covariance of predictor variables with class label 0\n",
    "cov2 = np.eye(data_dim) # covariance of predictor variables with class label 1\n",
    "\n",
    "\n",
    "# Sample data points \n",
    "X1 = np.random.multivariate_normal(mu1,cov1,size=Ndata1)\n",
    "Y1 = np.zeros(Ndata1)\n",
    "X2 = np.random.multivariate_normal(mu2,cov2,size=Ndata2)\n",
    "Y2 = np.ones(Ndata2)\n",
    "\n",
    "X = np.concatenate((X1,X2))\n",
    "Y = np.concatenate((Y1,Y2)) \n",
    "\n",
    "\n",
    "data = [X,Y] # data set in the format used in the logistic regression model below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the data set using the code the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndata = Y.shape[0]\n",
    "color_dict= {0:'red', 1 :'blue'}\n",
    "colors = [color_dict[Y[i]] for i in range(ndata)]\n",
    "plt.scatter(X[:,0],X[:,1], c=colors)\n",
    "plt.title('Data') \n",
    "plt.xlim([mu1[0]-3*np.sqrt(cov1[0,0]),+mu2[0]+3*np.sqrt(cov2[0,0])])\n",
    "plt.ylim([mu1[1]-3*np.sqrt(cov1[1,1]),+mu2[1]+3*np.sqrt(cov2[1,1])])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suitable logistic regression model specifying a posterior distribution $p(\\beta | x,y)$ of a form as described above is implemented in the class \"BayesianLogisticRegression\". The implemented regression model assumes a Gaussian prior. The code below initializes the classifier model we will use in the following. \n",
    " - Note: In the code $q$ should be interpreted as $\\beta$, i.e., the output of our sampler corresponds to trajectory of samples distributed according to $p(\\beta | x,y)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q0 = np.zeros(data_dim)\n",
    "p0 = np.random.normal(0., 1., data_dim)\n",
    "model = models.BayesianLogisticRegression(prior_mean = np.zeros(data_dim),\n",
    "                                               prior_cov = 100*np.eye(data_dim),\n",
    "                                               data = [X,Y], \n",
    "                                               q=q0, \n",
    "                                               p=p0\n",
    "                                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "- Set up an Ensbemble Quasi Newton sampler to sample from the posterior of the Bayesion logistic regression model specified above. It is recommended to use 10 replicas and a stepsize of $\\Delta t = .01$. Ensure that the number of time steps is sufficiently large that you get good exploration of your sampling space. (Hint: You may want to inspect the trace plot of $\\beta_1$ and $\\beta_2$ for this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "After you have obtained a sample trajectory using the Quasi Ensemble Newton method, you can ue the below code to plot the prediction $\\mathbb{P}[y_{pred} | x_{pred},x,y]$ for points on the grid $x_{pred} \\in$ grid. Where grid = xx1 $\\times$ xx2, with the vectors xx1 and xx2  as specified below. \n",
    "If computations take to much time you may want to adapt the values of Nx1, Nx2, Neval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Specify grid and number of sample points to be evaluated for prediction\n",
    "Nx1 = 50\n",
    "Nx2 = 50\n",
    "Neval = 100\n",
    "\n",
    "modthin = q_trajectory.shape[0]//Neval #Only every modthin-th sample of the trajectory used\n",
    "xx1 = np.linspace(mu1[0]-3*np.sqrt(cov1[0,0]),+mu2[0]+3*np.sqrt(cov2[0,0]),Nx1)\n",
    "xx2 = np.linspace(mu1[1]-3*np.sqrt(cov1[1,1]),+mu2[1]+3*np.sqrt(cov2[1,1]),Nx2)\n",
    "\n",
    "#Compute predictions\n",
    "z= np.zeros([len(xx1),len(xx2)])\n",
    "for i in range(len(xx1)):\n",
    "    for j in  range(len(xx2)):\n",
    "        x = np.array([xx1[i],xx2[j]])\n",
    "        z[i,j] = model.predict_from_sample(q_trajectory[::modthin,:model._dim], x)[0]\n",
    "\n",
    "#Plot figure        \n",
    "fig2, ax2 = plt.subplots()\n",
    "cax = ax2.pcolor(xx1, xx2, z.transpose(), cmap='RdBu', vmin=0, vmax=1)\n",
    "cbar = fig2.colorbar(cax)\n",
    "cbar.ax.set_ylabel('$\\mathbb{P}(Y = 1)$')\n",
    "ax2.scatter(X[:,0],X[:,1], c=colors)\n",
    "ax2.autoscale(enable=True, axis='both', tight=True)\n",
    "ax2.set_title(\"Prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus tasks \n",
    "\n",
    "You may want to explore how the training set determines the conditioning of the posterior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:anaconda]",
   "language": "python",
   "name": "conda-env-anaconda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
